{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_colab",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregory-ch/shap_flex_porting/blob/main/demo_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pCGiOaJxKOm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This module contains using user-defined trained models and prediction functions to compute approximate Shapley values for\n",
        "single models. \n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class shapFlex_plus:\n",
        "    def __init__(explain, reference = None, model, predict_function, target_features = None,\n",
        "                     causal = None, causal_weights = None, sample_size = None, use_future = None)):\n",
        "        self.explain = explain\n",
        "        self.reference = reference if reference else explain\n",
        "        self.model = model\n",
        "        predict_function = predict_function\n",
        "        self.target_features = target_features if target_features else explain.columns.tolist()\n",
        "        self.causal = causal if causal else None\n",
        "        self.causal_weights = causal_weights if causal_weights else None\n",
        "        self.sample_size = sample_size if sample_size else 60\n",
        "        self.use_future = use_future if target_features else False\n",
        "        \n",
        "        self.n_features = explain.shape[1]\n",
        "        n_instances = self.reference.shape[0]\n",
        "        \n",
        "      #def loop_over_monte_carlo_samples(i_size = self.samle_size, j_size = self.target_features):\n",
        "      #  for i in range(i_size):\n",
        "      #      for j in range(j_size):\n",
        "      #-------------------------------------------------------------------------\n",
        "      #вот здесь уже мое\n",
        "      @staticmethod\n",
        "      def unlist_df(data):\n",
        "        ulidsted_df = pd.Series(\n",
        "                    data,\n",
        "                    index=[\n",
        "                    index_col + index_row for index_col, index_row in itertools.product(\n",
        "                        [str(x) for x in range(data.shape[0])], \n",
        "                        [str(x) for x in data.columns])]\n",
        "                )\n",
        "        return unlisted_df\n",
        "        \n",
        "      def loop_over_monte_carlo_samples(self):\n",
        "        i_size = self.samlpe_size\n",
        "        j_size = self.target_features\n",
        "        for i in range(i_size):\n",
        "          reference_index = np.random.choice(np.arange(1, n_features+1), size=1, replace=False)\n",
        "          feature_indices_random = np.random.choice(np.arange(1, n_features+1), size=n_features, replace=False)\n",
        "          #reference это pd dataframe\n",
        "          feature_names_random = self.explain.columns[feature_indices_random].values\n",
        "          reference_instance = reference.iloc[reference_index, feature_indices_random]\n",
        "          #feature_indices_random это вектор индексов\n",
        "          explain_instances = explain.iloc[:, feature_indices_random]\n",
        "\n",
        "          for j in range(j_size):\n",
        "            target_feature_index = (self.explain.columns == self.target_features[j])\n",
        "            target_feature_index_shuffled = (self.explain[feature_indices_random].columns == self.target_features[j])\n",
        "\n",
        "            if self.target_features[j] in self.nodes:\n",
        "              #unlist как я понял, вытягивает все данные в один длинный вектор, присваивает индексы как название колонки + название строки\n",
        "              #предположу, что each_node_causes это pd.DataFrame()\n",
        "              target_feature_causes = self.unlist_df(\n",
        "                  #loc потому, что кажется target_features это не индекс\n",
        "                  each_node_causes.loc[:, target_features[j]]\n",
        "                  )\n",
        "              target_feature_is_caused_by = self.unlist_df(\n",
        "                  each_node_is_an_effect_from.loc[:, target_features[j]]\n",
        "                  )\n",
        "              \n",
        "              target_index = target_feature_index_shuffled\n",
        "              #отмечаем те значения feature_names_random которые равны последнему значению \n",
        "              #target_feature_is_caused_by. target_feature_is_caused_by вроде как вектор\n",
        "              #вернуться должно число. Если вдруг окажется, что датафрейм, -1 элемент будет строка, \n",
        "              #надо заменить на индексацию на iloc, == на .isin\n",
        "              causes_indices = (feature_names_random == target_feature_is_caused_by[-1])\n",
        "              effects_indices = (feature_names_random == target_feature_causes_these_features[-1])\n",
        "              sample_indices = feature_indices_random[~feature_indices_random.isin(\n",
        "                  np.concatenate([target_index, causes_indices, effects_indices]))]\n",
        "              #c() вроде как склеивает вектор(ы) и переменные\n",
        "              sample_real_indices = sample_indices[sample_indices < target_index]  # Not in causal diagram, feature data from 'explain'.\n",
        "              sample_fake_indices = sample_indices[sample_indices > target_index]  # Not in causal diagram, feature data from 'reference'.\n",
        "\n",
        "              feature_indices_real_causes_real_effects = np.concatenate([sample_real_indices, causes_indices, effects_indices, target_index, sample_fake_indices])\n",
        "              feature_indices_real_causes_fake_effects = np.concatenate([sample_real_indices, causes_indices, target_index, effects_indices, sample_fake_indices])\n",
        "              feature_indices_fake_causes_real_effects = np.concatenate([sample_real_indices, effects_indices, target_index, causes_indices, sample_fake_indices])\n",
        "              feature_indices_fake_causes_fake_effects = np.concatenate([sample_real_indices, target_index, causes_indices, effects_indices, sample_fake_indices])\n",
        "            \n",
        "            elif not target_features[j] in self.nodes:\n",
        "              explain_instance_real_target = explain_instances\n",
        "\n",
        "              # Only create a Frankenstein instance if the target is not the last feature and there is actually\n",
        "              # one or more features to the right of the target to replace with the reference.\n",
        "              if (target_feature_index_shuffled < n_features):\n",
        "                explain_instance_real_target.iloc[:, target_feature_index_shuffled + 1: n_features+1] = reference_instance.iloc[:, target_feature_index_shuffled+1: n_features+1]\n",
        "              \n",
        "              # These instances are otherwise the same as the Frankenstein instance created above with the\n",
        "              # exception that the target feature is now replaced with the target feature in the random reference\n",
        "              # instance. The difference in model predictions between these two Frankenstein instances is\n",
        "              # what gives us the stochastic Shapley value approximation.\n",
        "              explain_instance_fake_target = explain_instance_real_target\n",
        "              explain_instance_fake_target.iloc[:, target_feature_index_shuffled] = reference_instance[:, target_feature_index_shuffled]\n",
        "            \n",
        "            else:\n",
        "              pass\n"
      ],
      "metadata": {
        "id": "9-g4zu6ePJNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) собрать работающую функцию только с опциями по-умолчанию\n",
        "2) из аргументов не по-умолчанию пока реализованы только: reference \n",
        "3) текущий раздел работы строки: 125-397, в строках инициализируется функция сэмплирования, проходит по двум петлям цикла. След задача написать цикл для i. \n",
        "\n",
        "4) Написал начало цикла\n"
      ],
      "metadata": {
        "id": "Dvv8L5zPqeJf"
      }
    }
  ]
}