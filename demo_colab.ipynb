{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_colab",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregory-ch/shap_flex_porting/blob/main/demo_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pCGiOaJxKOm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This module contains using user-defined trained models and prediction functions to compute approximate Shapley values for\n",
        "single models. \n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class shapFlex_plus:\n",
        "    def __init__(self, explain,  model, predict_function, reference = None, target_features = None, \\\n",
        "                     causal = None, causal_weights = None, sample_size = None, use_future = None):\n",
        "        self.explain = explain\n",
        "        self.reference = reference if reference else explain\n",
        "        self.model = model\n",
        "        predict_function = predict_function\n",
        "        self.target_features = target_features if target_features else explain.columns.tolist()\n",
        "        self.causal = causal if causal else None\n",
        "        self.causal_weights = causal_weights if causal_weights else None\n",
        "        self.sample_size = sample_size if sample_size else 60\n",
        "        self.use_future = use_future if target_features else False\n",
        "        \n",
        "        self.n_features = self.explain.shape[1]\n",
        "        self.n_instances = self.reference.shape[0]\n",
        "\n",
        "        self.nodes = None\n",
        "\n",
        "    @staticmethod\n",
        "    def ulist_df(data):\n",
        "      unlisted_df = pd.Series(\n",
        "                  data,\n",
        "                  index=[\n",
        "                  index_col + index_row for index_col, index_row in itertools.product(\n",
        "                      [str(x) for x in range(data.shape[0])], \n",
        "                      [str(x) for x in data.columns])]\n",
        "              )\n",
        "      return unlisted_df\n",
        "      \n",
        "    def loop_over_monte_carlo_samples(self):\n",
        "      i_size = self.sample_size\n",
        "      j_size = len(self.target_features)\n",
        "      for i in range(i_size):\n",
        "        reference_index = np.random.choice(np.arange(0, self.n_features ), size=1, replace=False)\n",
        "        feature_indices_random = np.random.choice(np.arange(0, self.n_features), size=self.n_features, replace=False)\n",
        "        # r индексация стартует с 1 а питон с 0 поэтому нам нужно вычиать 1 или ставить по верхней границе индексы в зависимости от функции вызова\n",
        "        #reference это pd dataframe\n",
        "        feature_names_random = self.explain.columns[feature_indices_random].values\n",
        "        print(reference_index, feature_indices_random)\n",
        "        reference_instance = reference.iloc[reference_index, feature_indices_random]\n",
        "        #feature_indices_random это вектор индексов\n",
        "        explain_instances = explain.iloc[:, feature_indices_random]\n",
        "\n",
        "        for j in range(j_size):\n",
        "          target_feature_index =  self.explain.columns.get_loc(self.target_features[j])\n",
        "          target_feature_index_shuffled = list(self.explain.columns.values[feature_indices_random]).index(self.target_features[j])\n",
        "          # target_feature_index = (self.explain.columns == self.target_features[j])\n",
        "          # target_feature_index_shuffled = (self.explain.columns[feature_indices_random] == self.target_features[j])\n",
        "          \n",
        "          if self.target_features[j] in self.nodes:\n",
        "            #unlist как я понял, вытягивает все данные в один длинный вектор, присваивает индексы как название колонки + название строки\n",
        "            #предположу, что each_node_causes это pd.DataFrame()\n",
        "            target_feature_causes_these_features = self.unlist_df(\n",
        "                #loc потому, что кажется target_features это не индекс\n",
        "                each_node_causes.loc[:, self.target_features[j]]\n",
        "                )\n",
        "            target_feature_is_caused_by = self.unlist_df(\n",
        "                each_node_is_an_effect_from.loc[:, self.target_features[j]]\n",
        "                )\n",
        "            \n",
        "            target_index = target_feature_index_shuffled\n",
        "            #отмечаем те значения feature_names_random которые равны последнему значению \n",
        "            #target_feature_is_caused_by. target_feature_is_caused_by вроде как вектор\n",
        "            #вернуться должно число. Если вдруг окажется, что датафрейм, -1 элемент будет строка, \n",
        "            #надо заменить на индексацию на iloc, == на .isin\n",
        "            causes_indices = (feature_names_random == target_feature_is_caused_by[-1])\n",
        "            effects_indices = (feature_names_random == target_feature_causes_these_features[-1])\n",
        "            sample_indices = feature_indices_random[~feature_indices_random.isin(\n",
        "                np.concatenate([target_index, causes_indices, effects_indices]))]\n",
        "            #c() вроде как склеивает вектор(ы) и переменные\n",
        "            sample_real_indices = sample_indices[sample_indices < target_index]  # Not in causal diagram, feature data from 'explain'.\n",
        "            sample_fake_indices = sample_indices[sample_indices > target_index]  # Not in causal diagram, feature data from 'reference'.\n",
        "\n",
        "            feature_indices_real_causes_real_effects = np.concatenate([sample_real_indices, causes_indices, effects_indices, target_index, sample_fake_indices])\n",
        "            feature_indices_real_causes_fake_effects = np.concatenate([sample_real_indices, causes_indices, target_index, effects_indices, sample_fake_indices])\n",
        "            feature_indices_fake_causes_real_effects = np.concatenate([sample_real_indices, effects_indices, target_index, causes_indices, sample_fake_indices])\n",
        "            feature_indices_fake_causes_fake_effects = np.concatenate([sample_real_indices, target_index, causes_indices, effects_indices, sample_fake_indices])\n",
        "          \n",
        "          if not self.target_features[j] in self.nodes:\n",
        "            explain_instance_real_target = explain_instances\n",
        "\n",
        "            # Only create a Frankenstein instance if the target is not the last feature and there is actually\n",
        "            # one or more features to the right of the target to replace with the reference.\n",
        "            if (target_feature_index_shuffled < self.n_features):\n",
        "              explain_instance_real_target.iloc[:, target_feature_index_shuffled + 1: self.n_features + 1] =\\\n",
        "              reference_instance.iloc[:, target_feature_index_shuffled + 1: self.n_features + 1]\n",
        "            \n",
        "            # These instances are otherwise the same as the Frankenstein instance created above with the\n",
        "            # exception that the target feature is now replaced with the target feature in the random reference\n",
        "            # instance. The difference in model predictions between these two Frankenstein instances is\n",
        "            # what gives us the stochastic Shapley value approximation.\n",
        "            explain_instance_fake_target = explain_instance_real_target\n",
        "            explain_instance_fake_target.iloc[:, target_feature_index_shuffled] = reference_instance[:, target_feature_index_shuffled]\n",
        "          \n",
        "          else:\n",
        "\n",
        "            if self.target_features[j] in self.causal_nodes:\n",
        "              reference_instance_real_causes_fake_effects = reference_instance.iloc[:, feature_indices_real_causes_fake_effects]\n",
        "              explain_instance_real_causes_fake_effects_real_target = explain_instances.iloc[:, feature_indices_real_causes_fake_effects]\n",
        "              target_index_temp = (explain_instance_real_causes_fake_effects_real_target.columns.values == self.target_features[j])\n",
        "\n",
        "              if target_index_temp < self.n_features:\n",
        "                explain_instance_real_causes_fake_effects_real_target.iloc[:, target_index_temp + 1: self.n_features + 1] =\\\n",
        "                reference_instance_real_causes_fake_effects.iloc[:, target_index_temp + 1: self.n_features + 1]\n",
        "\n",
        "              explain_instance_real_causes_fake_effects_fake_target = explain_instance_real_causes_fake_effects_real_target\n",
        "              explain_instance_real_causes_fake_effects_fake_target.iloc[:, target_index_temp] =\\\n",
        "              reference_instance_real_causes_fake_effects.iloc[:, target_index_temp]\n",
        "              reference_instance_fake_causes_real_effects = reference_instance.iloc[:, feature_indices_fake_causes_real_effects]\n",
        "              explain_instance_fake_causes_real_effects_real_target_cause = explain_instances.iloc[:, feature_indices_fake_causes_real_effects]\n",
        "              target_index_temp = (explain_instance_fake_causes_real_effects_real_target_cause.columns.values == self.target_features[j])\n",
        "\n",
        "              if target_index_temp < self.n_features:\n",
        "                explain_instance_fake_causes_real_effects_real_target_cause.iloc[:, target_index_temp + 1: self.n_features + 1] =\\\n",
        "                reference_instance_fake_causes_real_effects[:, target_index_temp + 1: self.n_features]\n",
        "              \n",
        "              explain_instance_fake_causes_real_effects_fake_target_cause = explain_instance_fake_causes_real_effects_real_target_cause\n",
        "              explain_instance_fake_causes_real_effects_fake_target_cause.iloc[:, target_index_temp] =\\\n",
        "              reference_instance_fake_causes_real_effects.iloc[:, target_index_temp]\n",
        "\n",
        "            if self.target_features[j] in self.effect_nodes:\n",
        "              reference_instance_real_causes_fake_effects = reference_instance.iloc[:, feature_indices_real_causes_fake_effects]\n",
        "              explain_instance_real_causes_fake_effects_real_target_effect = explain_instances.iloc[:, feature_indices_real_causes_fake_effects]\n",
        "              target_index_temp = (explain_instance_real_causes_fake_effects_real_target_effect.columns.values == self.target_features[j])\n",
        "\n",
        "              if (target_index_temp < self.n_features):\n",
        "                explain_instance_real_causes_fake_effects_real_target_effect.iloc[:, target_index_temp + 1: self.n_features + 1] =\\\n",
        "                reference_instance_real_causes_fake_effects.iloc[:, target_index_temp + 1: self.n_features + 1]\n",
        "              \n",
        "              explain_instance_real_causes_fake_effects_fake_target_effect = explain_instance_real_causes_fake_effects_real_target_effect\n",
        "              explain_instance_real_causes_fake_effects_fake_target_effect.iloc[:, target_index_temp] =\\\n",
        "              reference_instance_real_causes_fake_effects.iloc[:, target_index_temp]\n",
        "              reference_instance_fake_causes_real_effects = reference_instance.iloc[:, feature_indices_fake_causes_real_effects]\n",
        "              explain_instance_fake_causes_real_effects_real_target = explain_instances.iloc[:, feature_indices_fake_causes_real_effects]\n",
        "              target_index_temp = (explain_instance_fake_causes_real_effects_real_target.columns.values == self.target_features[j])\n",
        "\n",
        "              if target_index_temp < self.n_features:\n",
        "                explain_instance_fake_causes_real_effects_real_target.iloc[:, target_index_temp + 1: self.n_features + 1] =\\\n",
        "                reference_instance_fake_causes_real_effects.iloc[:, target_index_temp + 1: self.n_features + 1]\n",
        "\n",
        "              explain_instance_fake_causes_real_effects_fake_target = explain_instance_fake_causes_real_effects_real_target\n",
        "              explain_instance_fake_causes_real_effects_fake_target.iloc[:, target_index_temp] =\\\n",
        "              reference_instance_fake_causes_real_effects.iloc[:, target_index_temp]\n",
        "\n",
        "          if not self.target_features[j] in self.nodes:\n",
        "            explain_instance_real_target = explain_instance_real_target.loc[:, explain.columns]\n",
        "            explain_instance_fake_target = explain_instance_fake_target.loc[:, explain.columns]\n",
        "            data_explain_instance = pd.concat([explain_instance_real_target, explain_instance_fake_target], axis=0)\n",
        "            #вот тут не совсем понятно, индекс это число или строка, индексы в data_explain_instance это числа или строки? в любом случае, при запуске можно починить\n",
        "            data_explain_instance[index] = np.tile(np.arange(1, explain.shape[1] + 1), 2) \n",
        "            data_explain_instance[feature_group] = np.tile(pd.Series(['real_target', 'fake_target']), explain.shape[0])\n",
        "            data_explain_instance[feature_name] = target_features[j]\n",
        "            data_explain_instance[causal] = 0\n",
        "            data_explain_instance[causal_type] = None\n",
        "\n",
        "          else:\n",
        "            if self.target_features[j] in self.causal_nodes:\n",
        "              explain_instance_real_causes_fake_effects_real_target =\\\n",
        "              explain_instance_real_causes_fake_effects_real_target.loc[:, explain.columns]\n",
        "              explain_instance_real_causes_fake_effects_fake_target =\\\n",
        "              explain_instance_real_causes_fake_effects_fake_target.loc[:, explain.columns]\n",
        "              explain_instance_fake_causes_real_effects_real_target_cause =\\\n",
        "              explain_instance_fake_causes_real_effects_real_target_cause.loc[:, explain.columns]\n",
        "              explain_instance_fake_causes_real_effects_fake_target_cause =\\\n",
        "              explain_instance_fake_causes_real_effects_fake_target_cause.loc[:, explain.columns]\n",
        "\n",
        "            if self.target_features[j] in self.effect_nodes:\n",
        "              explain_instance_real_causes_fake_effects_real_target_effect =\\\n",
        "              explain_instance_real_causes_fake_effects_real_target_effect.loc[:, explain.columns]\n",
        "              explain_instance_real_causes_fake_effects_fake_target_effect =\\\n",
        "              explain_instance_real_causes_fake_effects_fake_target_effect.loc[:, explain.columns]\n",
        "              explain_instance_fake_causes_real_effects_real_target =\\\n",
        "              explain_instance_fake_causes_real_effects_real_target.loc[:, explain.columns]\n",
        "              explain_instance_fake_causes_real_effects_fake_target =\\\n",
        "              explain_instance_fake_causes_real_effects_fake_target.loc[:, explain.columns]\n",
        "\n",
        "            if self.target_features[j] in self.causal_nodes:\n",
        "              data_explain_instance = pd.concat([\n",
        "                explain_instance_real_causes_fake_effects_real_target,\n",
        "                explain_instance_real_causes_fake_effects_fake_target,\n",
        "                explain_instance_fake_causes_real_effects_real_target_cause,\n",
        "                explain_instance_fake_causes_real_effects_fake_target_cause], axis=0\n",
        "              )\n",
        "              data_explain_instance[index] = np.tile(np.arange(1, explain.shape[0] + 1), 4)  # Four Frankenstein instances per explained instance.\n",
        "              data_explain_instance[feature_group] = np.tile(pd.Series([\"real_causes_fake_effects_real_target\", \"real_causes_fake_effects_fake_target\",\n",
        "                                                          \"fake_causes_real_effects_real_target_cause\", \"fake_causes_real_effects_fake_target_cause\"]),\n",
        "                                                        explain.shape[0])\n",
        "              data_explain_instance[causal_type] = \"target_is_a_cause\"\n",
        "\n",
        "            if self.target_features[j] in self.effect_nodes:\n",
        "              data_explain_instance <- pd.concat([\n",
        "                explain_instance_real_causes_fake_effects_real_target_effect,\n",
        "                explain_instance_real_causes_fake_effects_fake_target_effect,\n",
        "                explain_instance_fake_causes_real_effects_real_target,\n",
        "                explain_instance_fake_causes_real_effects_fake_target\n",
        "              ], axis=0)\n",
        "              data_explain_instance[index] = np.tile(np.arange(1, explain.shape[0] + 1), 4)  # Four Frankenstein instances per explained instance.\n",
        "              data_explain_instance[feature_group] = np.tile(pd.Series([\"real_causes_fake_effects_real_target_effect\", \"real_causes_fake_effects_fake_target_effect\",\n",
        "                                                          \"fake_causes_real_effects_real_target\", \"fake_causes_real_effects_fake_target\"]),\n",
        "                                                        explain.shape[0])\n",
        "              data_explain_instance[causal_type] = \"target_is_an_effect\"\n",
        "\n",
        "            if (self.target_features[j] in self.causal_nodes) and (self.target_features[j] in self.effect_nodes):\n",
        "              data_explain_instance = pd.concat([\n",
        "                explain_instance_real_causes_fake_effects_real_target,\n",
        "                explain_instance_real_causes_fake_effects_fake_target,\n",
        "                explain_instance_fake_causes_real_effects_real_target_cause,\n",
        "                explain_instance_fake_causes_real_effects_fake_target_cause,\n",
        "                explain_instance_real_causes_fake_effects_real_target_effect,\n",
        "                explain_instance_real_causes_fake_effects_fake_target_effect,\n",
        "                explain_instance_fake_causes_real_effects_real_target,\n",
        "                explain_instance_fake_causes_real_effects_fake_target\n",
        "              ], axis=0)\n",
        "              data_explain_instance[index] = np.tile(np.arange(1, explain.shape[0] + 1), 8)  # Eight Frankenstein instances per explained instance.\n",
        "              data_explain_instance[feature_group] = np.tile(pd.Series([\n",
        "                \"real_causes_fake_effects_real_target\", \"real_causes_fake_effects_fake_target\",  # Target is a causal node.\n",
        "                \"fake_causes_real_effects_real_target_cause\", \"fake_causes_real_effects_fake_target_cause\",  # Target is a causal node.\n",
        "                \"real_causes_fake_effects_real_target_effect\", \"real_causes_fake_effects_fake_target_effect\",  # Target is an effect node.\n",
        "                \"fake_causes_real_effects_real_target\", \"fake_causes_real_effects_fake_target\"  # Target is an effect node.\n",
        "                ]),\n",
        "              explain.shape[0])\n",
        "              data_explain_instance[causal_type] = np.tile(pd.Series([\n",
        "                \"target_is_a_cause\", \"target_is_a_cause\", \"target_is_a_cause\", \"target_is_a_cause\",\n",
        "                \"target_is_an_effect\", \"target_is_an_effect\", \"target_is_an_effect\", \"target_is_an_effect\"]\n",
        "              ),\n",
        "              explain.shape[0])\n",
        "            \n",
        "            data_explain_instance[feature_name] = target_features[j]\n",
        "            data_explain_instance[causal] = 1\n",
        "\n",
        "          data_explain_instance[sample] = i\n",
        "          data_explain_instance\n",
        "\n",
        "        data_sample_feature = pd.concat(data_sample_feature, axis=0)\n",
        "        data_sample_feature\n",
        "\n",
        "                        \n",
        "\n"
      ],
      "metadata": {
        "id": "9-g4zu6ePJNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) текущий раздел работы строки: 125-397, в строках инициализируется функция сэмплирования, проходит по двум петлям цикла. [закончено]\n",
        "2) Начало цикла [закончено]\n",
        "\n",
        "3) \"Франкенштейна\" cтр 172 -270 [закончено]\n",
        " \n",
        "4) Цикл i loop, j loop  стр 397 [закончено]\n",
        "\n",
        "5) написан код для инициализации объектов на вход в класс: модель, обработку данных, предикт функцию. Датасет в csv на гугл-диск кинул: https://drive.google.com/file/d/1ADJ2yNZum-quPW3bRWJ4iyEa2OoqlS18/view?usp=sharing, пока для простоты складывается в файлы колаба через drag-and-drop.\n",
        "\n",
        "6) Инициализация графа [закончено]\n",
        "\n",
        "7) В основном разделе R/shapFlex дошли до вызова функции predict_shapFlex на 401 стр., проверили инициализацию класса ShapFlex_plus начали отладку запуска loop_over_monte_carlo_samples()\n",
        "\n"
      ],
      "metadata": {
        "id": "Dvv8L5zPqeJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/data_adult.csv', index_col=0)\n",
        "encoder = OneHotEncoder()\n",
        "outcome_name = 'income'\n",
        "outcome_col = pd.Series(data.columns)[data.columns==outcome_name].index[0]\n",
        "model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
        "X, y = data.drop(outcome_name, axis=1), data[outcome_name].values\n",
        "X, y = pd.get_dummies(X, drop_first=True), np.array([1 if x == '<=50K' else 0 for x in y ]).ravel()\n",
        "model.fit(X, y)\n",
        "\n",
        "def predict_function(model, data):\n",
        "  #pd.DataFrame(model.predict_proba(X)).loc[:, 0][9] если запустить будет результат 0.98, что соответствует\n",
        "  #выводу для 9 номера который равен 0.98, неважно какой алгоритм, такая высокая степень уверенности\n",
        "  #позволяет идентифицировать выводимую колонку однозначно\n",
        "  X, y = data.drop(outcome_name, axis=1), data[outcome_name].values\n",
        "  X, y = pd.get_dummies(X, drop_first=True), np.array([1 if x == '<=50K' else 0 for x in y ]).ravel()\n",
        "  return pd.DataFrame(model.predict_proba(X)).loc[:, 0], X, y\n",
        "\n",
        "explain, reference = data.iloc[:300, :data.shape[1]-1], data.iloc[:, :data.shape[1]-1]\n",
        "sample_size = 60\n",
        "target_features = pd.Series([\"marital_status\", \"education\", \"relationship\",  \"native_country\",\n",
        "                     \"age\", \"sex\", \"race\", \"hours_per_week\"])\n",
        "causal = pd.DataFrame(\n",
        "  dict(cause=pd.Series([\"age\", \"sex\", \"race\", \"native_country\",\n",
        "              \"age\", \"sex\", \"race\", \"native_country\", \"age\",\n",
        "              \"sex\", \"race\", \"native_country\"]),\n",
        "  effect = pd.Series(np.concatenate([np.tile(\"marital_status\", 4), np.tile(\"education\", 4), np.tile(\"relationship\", 4)])))\n",
        ")"
      ],
      "metadata": {
        "id": "nsKJfNYYOjUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "Wq8NFfDkmQAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install igraph\n",
        "import igraph"
      ],
      "metadata": {
        "id": "I_mTNuEJcwr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1de64f5-f937-4917-856d-982f247a0a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting igraph\n",
            "  Downloading igraph-0.9.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph\n",
            "Successfully installed igraph-0.9.9 texttable-1.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "causal_graph = igraph.Graph.DataFrame(causal, directed=True)\n",
        "nodes = [v for v in causal_graph.vs]\n",
        "each_node_causes = {v: v.successors() for v in nodes if v.successors()}# надо уточнить, мб здесь не только \"прямые\" successors и predecessors ищутся \n",
        "each_node_is_an_effect_from = {v: v.predecessors() for v in nodes if v.predecessors()} # но и вообще все\n",
        "# имена, кажется, уже прописаны автоматически\n",
        "causal_nodes = [v['name'] for v in each_node_causes.keys()]\n",
        "effect_nodes = [v['name'] for v in each_node_is_an_effect_from.keys()]"
      ],
      "metadata": {
        "id": "mDvjYe2scxq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exmpl_of_test = shapFlex_plus(explain,  model, predict_function)\n",
        "# exmpl_of_test.explain.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAN8-Tct9_W8",
        "outputId": "f774fb96-da5a-442b-e48f-472ea01485f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'workclass', 'education', 'education_num', 'marital_status',\n",
              "       'occupation', 'relationship', 'race', 'sex', 'capital_gain',\n",
              "       'capital_loss', 'hours_per_week', 'native_country'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exmpl_of_test = shapFlex_plus(explain,  model, predict_function)\n",
        "exmpl_of_test.loop_over_monte_carlo_samples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "G1CgvQGw_DUk",
        "outputId": "e9aa0ee0-b797-4af7-ed2d-74e7075e612f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3] [12  6 11  7  2  0  1  9  5 10  3  8  4]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-f3df0365af07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexmpl_of_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshapFlex_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexmpl_of_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop_over_monte_carlo_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-bd481a24c104>\u001b[0m in \u001b[0;36mloop_over_monte_carlo_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0;31m# target_feature_index_shuffled = (self.explain.columns[feature_indices_random] == self.target_features[j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;31m#unlist как я понял, вытягивает все данные в один длинный вектор, присваивает индексы как название колонки + название строки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m#предположу, что each_node_causes это pd.DataFrame()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
          ]
        }
      ]
    }
  ]
}